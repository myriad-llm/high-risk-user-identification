# lightning.pytorch==2.3.1
seed_everything: 123
ckpt_path: null
trainer:
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "bilstm_attention"
      save_dir: "./wandb/bilstm_attention"
      log_model: true
  max_epochs: 100
  default_root_dir: ./lightning_logs/bilstm_attention
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: 'valid_BinaryF1Score'
        mode: 'max'
        save_top_k: 10
        filename: '{epoch}-{valid_BinaryF1Score:.3f}'
        verbose: false
    - class_path: callbacks.LSTMPredictionWriter
      init_args:
        output_dir: './predictions/bilstm_attention'
        write_interval: "epoch"
model:
  class_path: models.BiLSTMWithImprovedAttention
  init_args:
    input_size: 1908
    hidden_size: 128
    num_layers: 4
    num_classes: 2
    attention_size: 32
    dropout_rate: 0.5
    num_heads: 6
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 1e-3
        weight_decay: 0.01
data:
  class_path: datasets.CallRecordsDataModule
  init_args:
    data_dir: './data'
    batch_size: 32
    seed: 12
    non_seq: false
    num_workers: 1