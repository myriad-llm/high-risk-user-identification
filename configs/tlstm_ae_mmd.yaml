# lightning.pytorch==2.3.1
seed_everything: 123
ckpt_path: null
trainer:
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "tlstm_autoencoder_mmd"
      save_dir: "./wandb/${trainer.logger.init_args.project}"
      log_model: true
  max_epochs: 1000
  default_root_dir: "./lightning_logs/${trainer.logger.init_args.project}"
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_loss"
        mode: "min"
        save_top_k: 10
        filename: "{epoch}-{val_loss:.3f}"
        verbose: false
    - class_path: callbacks.LSTMPredictionWriter
      init_args:
        output_dir: "./predictions/${trainer.logger.init_args.project}"
        write_interval: "epoch"
model:
  class_path: models.TimeLSTMAutoEncoder_MMD
  init_args:
    input_size: 1868
    hidden_dim1_e: 512
    output_dim1_e: ${model.init_args.hidden_dim1_e}
    encoded_dim: 128
    output_dim2_e: ${model.init_args.encoded_dim}
    hidden_dim2_d: ${model.init_args.hidden_dim1_e}
    output_dim1_d: ${model.init_args.encoded_dim}
    decoded_dim: ${model.init_args.input_size}
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.003
        weight_decay: 0.01
data:
  class_path: datasets.aug_bak.CallRecordsAugDataModule
  init_args:
    data_dir: "./data"
    batch_size: 32
    seed: 12
    non_seq: false
    num_workers: 4
    time_div: 360
    mask_rate: 0.0
    aug_ratio: 0.0
    aug_times: 0
