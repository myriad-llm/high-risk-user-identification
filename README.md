## high-risk-user-identification

### Datasets

#### CallRecords

```shell
mkdir -p data/CallRecords/raw
```

Put `trainSet_ans.csv`, `testSet_res.csv` and `validationSet_res.csv` files
at `data/CallRecords/raw/` directory.

At the first time to run this code, the raw data will be preprocessed and saved
at `data/CallRecords/processed/` directory.
After that, no longer need to process the raw data again.

dataset `call_records` return:

seq: BATCH * SEQ_LEN * FEATURE

time_diff: BATCH * SEQ_LEN

labels: BATCH * CLASS_NUM  

#### CallRecordsV2

This is for model `CallRecordsV2DataModule`

```shell
mkdir -p data/CallRecordsV2/raw
```

Put `trainSet_ans.csv`, `testSet_res.csv` and `validationSet_res.csv` files
at `data/CallRecordsV2/raw/` directory.

### Use

Train:

```shell
CUDA_VISIBLE_DEVICES=2 python main.py fit --config ./configs/<name>.yaml
```

The logs and checkpoint will be saved at `./wandb/`

Predict:

Before predict, you need to modify the `ckpt_path` in the config file to determine which checkpoint to use.

```shell
CUDA_VISIBLE_DEVICES=2 python main.py predict --config ./configs/<name>.yaml
```

### Dev

#### Models and DataModules

if you want to make the `feature_dim` in the config file self-adapting to the dataset, you must define the `feature_dim` attribute in the datamodule to return the feature number.

```python
class CallRecordsDataModule(pl.LightningDataModule):
    @property
    def feature_dim(self):
        ...
        return feature_dim
```

Correspondingly, you must add the `input_size` parameter in the model.

```python
class MyModel(LightningModule):
    def __init__(self, input_size, ...):
        super().__init__()
        self.input_size = input_size
        ...
```

If `feature_dim` not exist in the config file, it will return an error like this:

```shell
 "C:\Users\shiwenbo\mambaforge\envs\pytorch\lib\site-packages\torch\nn\modules\rnn.py", line 117, in __init__
w_ih = Parameter(torch.empty((gate_size, layer_input_size), **factory_kwargs))
TypeError: empty(): argument 'size' failed to unpack the object at pos 2 with error "type must be tuple of ints,but got NoneType"
```

And, the `input_size` parameter in the model can't add `Type Annotations` for now, otherwise it will raise an error.

Just don't do this:

```python
class MyModel(LightningModule):
    def __init__(self, input_size: int, ...):
        ...
```

#### Generate Config File

```shell
python main.py <subcommand> --model models.<model_class> --datamodule datamodules.<datamodule_class> --config ./configs/<name>.yaml
```

use `python main.py --help` to see the subcommands.

example:

```shell
python main.py fit --model models.LSTM --datamodule datamodules.CallRecordsDataModule --print_config > ./configs/<name>.yaml
```

To keep the config file simple, delete the unnecessary parameters in the yaml file generated by `--print_config`, and reuse the config file generated by `fit` as the config file for `predict`.

ps: [Models and DataModules](#models-and-datamodules) just mentioned that `input_size` can be self-adapting according to `feature_dim`, so the `input_size` in the config file generated by `--print_config` is useless and can be deleted. And, when training the model from scratch, the `ckpt_path` in the config file must be set to `null`.

#### How to resume training

1. Change `ckpt_path` in the config file to the checkpoint you want to resume.
2. Run `python main.py fit --config ./configs/<name>.yaml`

#### How to predict with specific checkpoint

1. Change `ckpt_path` in the config file to the checkpoint you want to use.
2. Run `python main.py predict --config ./configs/<name>.yaml`
