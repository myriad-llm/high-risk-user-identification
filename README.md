## high-risk-user-identification

### Datasets

#### CallRecords

```shell
mkdir -p data/CallRecords/raw
```

Put `trainSet_ans.csv`, `testSet_res.csv` and `validationSet_res.csv` files
at `data/CallRecords/raw/` directory.

At the first time to run this code, the raw data will be preprocessed and saved
at `data/CallRecords/processed/` directory.
After that, no longer need to process the raw data again.

dataset `call_records` return:

seq: BATCH * SEQ_LEN * FEATURE

time_diff: BATCH * SEQ_LEN

labels: BATCH * CLASS_NUM  

#### CallRecordsV2

This is for model `CallRecordsV2DataModule`

```shell
mkdir -p data/CallRecordsV2/raw
```

Put `trainSet_ans.csv`, `testSet_res.csv` and `validationSet_res.csv` files
at `data/CallRecordsV2/raw/` directory.

#### CallRecordsAug

functions:

1. data augmentation
2. entity embedding

```shell
mkdir -p data/CallRecordsAug/raw
```

This is for the models that use `models/common/embedding.py`, which can perform Entity Embedding on the categorical variables in the raw data.

Will additionally generate a `embedding_items.pkl` file in the `data/CallRecordsAug/preprocessed/` directory.

Because of changing onehot to more dense embedding representation, it has faster training speed, smaller memory requirement and supports higher data augmentation times.

This datamodule should be used with the script `main_embedding.py` which has a feature that can add the `embedding_items.pkl` file to the config file automatically.

### Use

Train:

```shell
CUDA_VISIBLE_DEVICES=2 python main.py fit --config ./configs/<name>.yaml
```

The logs and checkpoint will be saved at `./wandb/`

Predict:

Before predict, you need to modify the `ckpt_path` in the config file to determine which checkpoint to use.

```shell
CUDA_VISIBLE_DEVICES=2 python main.py predict --config ./configs/<name>.yaml
```

### Dev

#### Models and DataModules

if you want to make the `feature_dim` in the config file self-adapting to the dataset, you must define the `feature_dim` attribute in the datamodule to return the feature number.

```python
class CallRecordsDataModule(pl.LightningDataModule):
    @property
    def feature_dim(self):
        ...
        return feature_dim
```

Correspondingly, you must add the `input_size` parameter in the model.

```python
class MyModel(LightningModule):
    def __init__(self, input_size, ...):
        super().__init__()
        self.input_size = input_size
        ...
```

If `feature_dim` not exist in the config file, it will return an error like this:

```shell
 "C:\Users\shiwenbo\mambaforge\envs\pytorch\lib\site-packages\torch\nn\modules\rnn.py", line 117, in __init__
w_ih = Parameter(torch.empty((gate_size, layer_input_size), **factory_kwargs))
TypeError: empty(): argument 'size' failed to unpack the object at pos 2 with error "type must be tuple of ints,but got NoneType"
```

And, the `input_size` parameter in the model can't add `Type Annotations` for now, otherwise it will raise an error.

Just don't do this:

```python
class MyModel(LightningModule):
    def __init__(self, input_size: int, ...):
        ...
```

#### Generate Config File

```shell
python main.py <subcommand> --model models.<model_class> --datamodule datamodules.<datamodule_class> --config ./configs/<name>.yaml
```

use `python main.py --help` to see the subcommands.

example:

```shell
python main.py fit --model models.LSTM --datamodule datamodules.CallRecordsDataModule --print_config > ./configs/<name>.yaml
```

To keep the config file simple, delete the unnecessary parameters in the yaml file generated by `--print_config`, and reuse the config file generated by `fit` as the config file for `predict`.

ps: [Models and DataModules](#models-and-datamodules) just mentioned that `input_size` can be self-adapting according to `feature_dim`, so the `input_size` in the config file generated by `--print_config` is useless and can be deleted. And, when training the model from scratch, the `ckpt_path` in the config file must be set to `null`.

#### How to resume training

1. Change `ckpt_path` in the config file to the checkpoint you want to resume.
2. Run `python main.py fit --config ./configs/<name>.yaml`

#### How to predict with specific checkpoint

1. Change `ckpt_path` in the config file to the checkpoint you want to use.
2. Run `python main.py predict --config ./configs/<name>.yaml`

### Hyperparameter Tuning

use the `Optuna` library to tune hyperparameters [Optuna](https://optuna.org/).

The hyperparameters optimization use the bayesian optimization algorithm.

```shell
python main_embedding_opt.py --config ./configs/<name>.yaml
```

Basic `config` file is still read from the `yaml` file, and the hyperparameter search range is defined in the `objective` function in the `main_embedding_opt.py` file. The settings in `objective` will override the settings in the `config` file. If you want to change the hyperparameter search range to fit your model, you can modify the `objective` function in the `main_embedding_opt.py` file like this:

```python
hidden_dim = trial.suggest_categorical("hidden_dim", [2, 4, 8, 16, 32, 64])
num_layers = trial.suggest_categorical("num_layers", [1, 2, 3])
batch_size = trial.suggest_categorical("batch_size", [64, 128, 256, 512])
lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
max_epochs = 150

cli_update = {
    "data": {
        "init_args": {
            "batch_size": batch_size,
        }
    },
    "model": {
        "init_args": {
            "hidden_size": hidden_dim,
            "num_layers": num_layers,
            "optimizer": {"init_args": {"lr": lr}},
        }
    },
    "trainer": {"max_epochs": max_epochs},
}
```

- [ ] Make the setting of hyperparameter range in `objective` bind to the model, so that the setting of hyperparameters is more flexible.

ps: Now, the `objective` function is just for `lstm` model as an example. Don't commit the changed `main_embedding_opt.py` file to the repository.
